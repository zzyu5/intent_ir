import torch
import triton
import triton.language as tl


@triton.jit
def gather2d_kernel(
    inp_ptr,
    row_idx_ptr,
    col_idx_ptr,
    out_ptr,
    M,
    N,
    L,
    BLOCK: tl.constexpr,
):
    pid = tl.program_id(0)
    offs = pid * BLOCK + tl.arange(0, BLOCK)
    m = offs < L
    rows = tl.load(row_idx_ptr + offs, mask=m, other=0).to(tl.int32)
    cols = tl.load(col_idx_ptr + offs, mask=m, other=0).to(tl.int32)
    # Assume indices are in-bounds (generated by the reference runner).
    x = tl.load(inp_ptr + rows.to(tl.int64) * N + cols.to(tl.int64), mask=m, other=0.0).to(tl.float32)
    tl.store(out_ptr + offs, x, mask=m)


def gather2d(inp: torch.Tensor, row_idx: torch.Tensor, col_idx: torch.Tensor) -> torch.Tensor:
    if inp.dtype != torch.float32:
        raise TypeError("gather2d expects float32 inp")
    if inp.ndim != 2:
        raise ValueError("gather2d expects inp rank-2")
    if row_idx.dtype != torch.int32 or col_idx.dtype != torch.int32:
        raise TypeError("gather2d expects int32 indices")
    if row_idx.ndim != 1 or col_idx.ndim != 1:
        raise ValueError("gather2d expects rank-1 indices")
    if int(row_idx.shape[0]) != int(col_idx.shape[0]):
        raise ValueError("gather2d expects row_idx and col_idx with same length")
    m, n = inp.shape
    l = int(row_idx.shape[0])
    out = torch.empty((l,), device=inp.device, dtype=torch.float32)
    grid = lambda meta: (triton.cdiv(l, meta["BLOCK"]),)
    gather2d_kernel[grid](inp, row_idx, col_idx, out, int(m), int(n), int(l), BLOCK=256)
    return out


__all__ = ["gather2d_kernel", "gather2d"]
