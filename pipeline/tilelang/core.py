"""
TileLang MVP full pipeline runner (PR#9).

This mirrors the Triton pipeline shape, but uses the TileLang adapter:
  TileLang DSL -> CertificateV2 -> obligations -> contract -> LLM->IntentIR -> diff.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np

from frontends.common.contract_v2 import evaluate_contract_v2
from frontends.common.obligations import evaluate_obligations
from pipeline import registry as pipeline_registry
from pipeline.interfaces import FrontendConstraints
from verify.diff_runner import run_diff
from verify.gen_cases import GeneratedCases, TestCase, generate_cases_split
from verify.metamorphic import run_bounded_exhaustive, run_metamorphic_suite
from verify.mutation import run_mutation_kill

from intent_ir.llm import LLMIntentHub
from intent_ir.macros import enrich_intent_macros
from intent_ir.parser import CandidateIntent
from intent_ir.ir.printer_mlir_like import print_mlir_like

from frontends.tilelang.runtime import infer_written_global_buffers, run_tilelang_kernel_io

from kernels.tilelang.ops.any_kernel_dim import any_kernel_dim_spec
from kernels.tilelang.ops.groupnorm import group_norm_kernel_spec
from kernels.tilelang.ops.softmax_inner import softmax_inner_spec
from kernels.tilelang.ops.layernorm import layer_norm_persistent_spec
from kernels.tilelang.ops.upsample_bicubic2d_aa import upsample_bicubic2d_aa_spec
from kernels.tilelang.ops._attn_fwd import attn_fwd_spec
from kernels.tilelang.ops.gemm import gemm_spec
from kernels.tilelang.ops.spec import TileLangKernelSpec


ROOT = Path(__file__).resolve().parents[2]
_LLM_HUB = LLMIntentHub()


def _ensure_schedule_tilelang(intent, spec) -> None:
    """
    Keep schedule visible in IntentIR.
    - Prefer schedule produced by LLM.
    - If missing, fall back to the spec's deterministic intent schedule (sketch).
    """
    if getattr(intent, "schedule", None) is not None:
        return
    try:
        det = spec.intent_builder()
        intent.schedule = det.schedule
    except Exception:
        return


def _buffer_param_names(prim_func) -> List[str]:
    try:
        from tvm import tir  # noqa: PLC0415

        if not isinstance(prim_func, tir.PrimFunc):
            return []
        out: List[str] = []
        for p in list(prim_func.params):
            if p in prim_func.buffer_map:
                out.append(str(prim_func.buffer_map[p].name))
        return out
    except Exception:
        return []


def _run_tilelang_ref(spec: TileLangKernelSpec, case: TestCase) -> Dict[str, np.ndarray]:
    """
    Execute the real TileLang kernel to produce a reference IO snapshot.

    Inputs are generated by the spec's numpy reference runner (for deterministic RNG),
    but outputs come from executing `spec.prim_func` via tilelang.compile.
    """
    ref_io = spec.runner(case)
    prim_func = spec.prim_func
    buf_names = _buffer_param_names(prim_func)
    written = set(infer_written_global_buffers(prim_func))
    inputs_np = {k: np.asarray(v) for k, v in ref_io.items() if k in buf_names and k not in written}
    io = run_tilelang_kernel_io(prim_func, bindings=dict(case.shapes), inputs_np=inputs_np)
    # Keep scalar inputs from reference runner (e.g., eps) if present.
    for k, v in ref_io.items():
        if k not in io and k not in buf_names:
            io[k] = np.asarray(v)
    return io


def mvp_kernel_specs() -> List[TileLangKernelSpec]:
    """
    PR#9 original MVP: one anchor-strong kernel to prove the pipeline can host TileLang.
    """
    return [gemm_spec()]


def default_kernel_specs() -> List[TileLangKernelSpec]:
    """
    Regression suite: mirror Triton's 6 representative kernels (by name).
    """
    return [
        any_kernel_dim_spec(),
        group_norm_kernel_spec(),
        attn_fwd_spec(),
        softmax_inner_spec(),
        layer_norm_persistent_spec(),
        upsample_bicubic2d_aa_spec(),
    ]


def run_pipeline_for_spec(
    spec: TileLangKernelSpec,
    *,
    out_dir: Path,
    cases_limit: int = 8,
    stage_c: bool = True,
    mutation_kill: bool = True,
    use_llm: bool = True,
    use_tilelang_runtime: bool = True,
    llm_model: Optional[str] = None,
) -> Dict[str, object]:
    report: Dict[str, object] = {"kernel": spec.name, "frontend": "tilelang"}
    out_dir.mkdir(parents=True, exist_ok=True)
    adapter = pipeline_registry.get("tilelang")

    # 1) Descriptor / facts / constraints / certificate
    desc = adapter.build_descriptor(spec)
    desc.meta["artifact_dir"] = str(out_dir)
    (out_dir / f"{spec.name}.tilelang_tir.py").write_text(desc.source_text, encoding="utf-8")
    report["descriptor"] = desc.to_json_dict()

    desc = adapter.ensure_artifacts(desc, spec)
    facts = adapter.extract_facts(desc)
    constraints: FrontendConstraints = adapter.extract_constraints(desc, facts)
    cert_v2 = adapter.build_certificate(desc, facts, constraints)

    obligations = evaluate_obligations(desc, cert_v2)
    cert_v2.semantic_facts["obligations"] = [o.to_json_dict() for o in obligations]
    contract = evaluate_contract_v2(desc, cert_v2, obligations, constraints=constraints)

    report["certificate_v2"] = cert_v2.to_json_dict()
    (out_dir / f"{spec.name}.certificate_v2.json").write_text(json.dumps(report["certificate_v2"], indent=2), encoding="utf-8")
    report["contract"] = {
        "level": contract.level,
        "reasons": list(contract.reasons),
        "assumptions": list(contract.assumptions),
        "signals": dict(contract.signals),
    }
    (out_dir / f"{spec.name}.contract.json").write_text(json.dumps(report["contract"], indent=2), encoding="utf-8")

    # 2) IntentIR: LLM (default) or deterministic fallback (tests/CI).
    if use_llm:
        cand = _LLM_HUB.lift(desc, model=llm_model)
        enrich_intent_macros(cand.intent)
        _ensure_schedule_tilelang(cand.intent, spec)
        report["llm_trace"] = dict(cand.llm_trace or {})
    else:
        intent = spec.intent_builder()
        cand = CandidateIntent(intent=intent, llm_trace={"provider": "tilelang_deterministic"})

    (out_dir / f"{spec.name}.intentir.mlir").write_text(print_mlir_like(cand.intent), encoding="utf-8")
    report["intent"] = cand.intent.to_json_dict()

    # 3) Stage B: cases + diff
    use_rt_ref = bool(use_tilelang_runtime) and (contract.level != "OUT_OF_SCOPE")
    run_ref_fn = (lambda c: _run_tilelang_ref(spec, c)) if use_rt_ref else spec.runner
    cases_pack: GeneratedCases = generate_cases_split(
        cand.intent,
        constraints=constraints,
        limit=int(cases_limit),
        seed=0,
        axes=list(spec.vary_axes),
        exclude_axes=list(spec.exclude_axes or []),
        assumptions=list(contract.assumptions),
        base_shapes=dict(spec.canonical_shapes),
    )
    cases_in = list(cases_pack.in_contract)
    cases_out = list(cases_pack.out_of_contract)
    report["cases"] = {"in_contract": [dict(c.shapes) for c in cases_in], "out_of_contract": [dict(c.shapes) for c in cases_out]}

    diffs_in, _ = run_diff(cand.intent, run_ref_fn, cases_in)
    report["diff"] = {
        "ok": bool(diffs_in and all(d.ok for d in diffs_in)),
        "results": [
            {
                "ok": bool(d.ok),
                "max_abs_err": float(d.max_abs_err),
                "max_rel_err": float(d.max_rel_err),
                "first_bad_index": ([int(x) for x in d.first_bad_index] if d.first_bad_index is not None else None),
                "summary": str(d.summary),
            }
            for d in diffs_in
        ],
    }

    if stage_c:
        base_case = TestCase(shapes=dict(spec.canonical_shapes), dtypes={}, seed=0)
        meta = run_metamorphic_suite(spec.name, cand.intent, run_ref_fn, base_case=base_case)
        bounded = run_bounded_exhaustive(spec.name, cand.intent, run_ref_fn, max_cases=64)
        report["stage_c"] = {
            "metamorphic": {
                "ok": bool(meta.ok),
                "results": [{"relation": r.relation, "ok": bool(r.ok), "detail": r.detail} for r in meta.results],
            },
            "bounded_exhaustive": {
                "ok": bool(bounded.ok),
                "checked": int(bounded.checked),
                "total": int(bounded.total),
                "detail": bounded.detail,
                "first_failure": (dict(bounded.first_failure_case.shapes) if bounded.first_failure_case else None),
                "first_failure_summary": bounded.first_failure_summary,
            },
        }
    else:
        report["stage_c"] = {"skipped": True}

    if mutation_kill:
        base_case = TestCase(shapes=dict(spec.canonical_shapes), dtypes={}, seed=0)
        diff_cases = cases_in[:2] if cases_in else [base_case]
        metamorphic_base = cases_in[0] if cases_in else base_case
        mut = run_mutation_kill(
            spec.name,
            intent=cand.intent,
            run_ref_fn=run_ref_fn,
            diff_cases=diff_cases,
            metamorphic_base_case=metamorphic_base,
            static_validate_fn=None,
            n_mutants=8,
            seed=0,
        )
        report["mutation_kill"] = {
            "kill_rate": float(mut.kill_rate),
            "total": int(mut.total),
            "killed": int(mut.killed),
            "survived": int(mut.survived),
            "killed_by_stage": dict(mut.killed_by_stage),
            "outcomes": [
                {"mutant_id": o.mutant_id, "killed_by": o.killed_by, "detail": o.detail, "diff_summary": o.diff_summary}
                for o in mut.outcomes
            ],
        }
    else:
        report["mutation_kill"] = {"skipped": True}

    # Persist baseline IO for Task6 tools (remote RVV / backend codegen smoke).
    try:
        baseline_case = TestCase(shapes=dict(spec.canonical_shapes), dtypes={}, seed=0)
        baseline_io = run_ref_fn(baseline_case)
        try:
            from verify.diff_runner import _with_io_aliases as _with_io_aliases_for_diff

            baseline_io = _with_io_aliases_for_diff(cand.intent, baseline_io)
        except Exception:
            pass
        total_bytes = 0
        for v in baseline_io.values():
            arr = np.asarray(v)
            total_bytes += int(arr.size) * int(arr.dtype.itemsize)
        if total_bytes <= 16 * 1024 * 1024:
            npz_path = out_dir / f"{spec.name}.baseline.npz"
            np.savez_compressed(npz_path, **{k: np.asarray(v) for k, v in baseline_io.items()})
            report["baseline"] = {
                "shapes": dict(baseline_case.shapes),
                "seed": int(baseline_case.seed),
                "npz_path": str(npz_path),
                "keys": sorted(list(baseline_io.keys())),
                "bytes": int(total_bytes),
            }
        else:
            report["baseline"] = {
                "shapes": dict(baseline_case.shapes),
                "seed": int(baseline_case.seed),
                "npz_path": None,
                "keys": sorted(list(baseline_io.keys())),
                "bytes": int(total_bytes),
                "skipped": "baseline too large to cache (over 16MB)",
            }
    except Exception as e:
        report["baseline"] = {"error": f"{type(e).__name__}: {e}"}

    return report


__all__ = ["TileLangKernelSpec", "mvp_kernel_specs", "default_kernel_specs", "run_pipeline_for_spec"]
